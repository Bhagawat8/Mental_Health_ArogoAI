{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10673445,"sourceType":"datasetVersion","datasetId":6611154}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T10:40:07.013872Z","iopub.execute_input":"2025-02-07T10:40:07.014175Z","iopub.status.idle":"2025-02-07T10:40:07.361395Z","shell.execute_reply.started":"2025-02-07T10:40:07.014146Z","shell.execute_reply":"2025-02-07T10:40:07.360544Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/arogoai-assignment/professional_mental.csv\n/kaggle/input/arogoai-assignment/student_mental.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pytorch_tabnet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T10:40:11.589104Z","iopub.execute_input":"2025-02-07T10:40:11.589569Z","iopub.status.idle":"2025-02-07T10:40:16.037328Z","shell.execute_reply.started":"2025-02-07T10:40:11.589531Z","shell.execute_reply":"2025-02-07T10:40:16.036489Z"}},"outputs":[{"name":"stdout","text":"Collecting pytorch_tabnet\n  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.26.4)\nRequirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.2.2)\nRequirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.13.1)\nRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (2.5.1+cu121)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pytorch_tabnet) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pytorch_tabnet) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pytorch_tabnet) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pytorch_tabnet) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pytorch_tabnet) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->pytorch_tabnet) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch_tabnet) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch_tabnet) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->pytorch_tabnet) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->pytorch_tabnet) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->pytorch_tabnet) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->pytorch_tabnet) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->pytorch_tabnet) (2024.2.0)\nDownloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pytorch_tabnet\nSuccessfully installed pytorch_tabnet-4.1.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T10:40:18.731004Z","iopub.execute_input":"2025-02-07T10:40:18.731296Z","iopub.status.idle":"2025-02-07T10:40:22.424882Z","shell.execute_reply.started":"2025-02-07T10:40:18.731271Z","shell.execute_reply":"2025-02-07T10:40:22.424225Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# TabNet code for professional","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetClassifier\n\ndata_path = \"/kaggle/input/mental-health-arogoai/professional_mental.csv\"\ndf = pd.read_csv(data_path)\n\nif \"Unnamed: 0.1\" in df.columns:\n    df.drop([\"Unnamed: 0.1\"], axis=1, inplace=True)\nif \"Unnamed: 0\" in df.columns:\n    df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n\ntarget_col = \"Depression\"\nX = df.drop(target_col, axis=1).values\ny = df[target_col].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nclf = TabNetClassifier(\n    input_dim=13,\n    device_name=device.type,\n    \n    n_d=8,              \n    n_a=8,              \n    n_steps=5,          \n    gamma=1.3,           \n    lambda_sparse=1e-3, \n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=0.01),\n    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n    mask_type='sparsemax', \n)\n\nmax_epochs = 100\nbatch_size = 1024\nvirtual_batch_size = 128\n\nclf.fit(\n    X_train, y_train,\n    eval_set=[(X_train, y_train), (X_test, y_test)],\n    eval_name=['train', 'valid'],\n    eval_metric=['accuracy'],\n    max_epochs=max_epochs,\n    patience=15,  \n    batch_size=batch_size,\n    virtual_batch_size=virtual_batch_size,\n    num_workers=0,\n    drop_last=False\n)\n\nmodel_path = \"network.zip\"\nclf.save_model(model_path)\nprint(f\"Model saved to {model_path}\")\n\npreds = clf.predict(X_test)\npred_probs = clf.predict_proba(X_test)\n\nacc = accuracy_score(y_test, preds)\nf1 = f1_score(y_test, preds, average=\"weighted\")  \nprint(f\"Test Accuracy: {acc:.4f}\")\nprint(f\"Test F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T20:59:49.141796Z","iopub.execute_input":"2025-02-06T20:59:49.142183Z","iopub.status.idle":"2025-02-06T21:11:55.902995Z","shell.execute_reply.started":"2025-02-06T20:59:49.142154Z","shell.execute_reply":"2025-02-06T21:11:55.902096Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n","output_type":"stream"},{"name":"stdout","text":"epoch 0  | loss: 0.27158 | train_accuracy: 0.94484 | valid_accuracy: 0.94587 |  0:00:09s\nepoch 1  | loss: 0.14491 | train_accuracy: 0.95032 | valid_accuracy: 0.95169 |  0:00:18s\nepoch 2  | loss: 0.13575 | train_accuracy: 0.95214 | valid_accuracy: 0.95337 |  0:00:27s\nepoch 3  | loss: 0.13113 | train_accuracy: 0.95248 | valid_accuracy: 0.95315 |  0:00:36s\nepoch 4  | loss: 0.12915 | train_accuracy: 0.95378 | valid_accuracy: 0.95368 |  0:00:45s\nepoch 5  | loss: 0.12547 | train_accuracy: 0.95435 | valid_accuracy: 0.95626 |  0:00:54s\nepoch 6  | loss: 0.12353 | train_accuracy: 0.95547 | valid_accuracy: 0.95706 |  0:01:03s\nepoch 7  | loss: 0.11986 | train_accuracy: 0.95599 | valid_accuracy: 0.95764 |  0:01:12s\nepoch 8  | loss: 0.11849 | train_accuracy: 0.95686 | valid_accuracy: 0.95781 |  0:01:22s\nepoch 9  | loss: 0.11948 | train_accuracy: 0.95684 | valid_accuracy: 0.95715 |  0:01:31s\nepoch 10 | loss: 0.11624 | train_accuracy: 0.95655 | valid_accuracy: 0.95732 |  0:01:40s\nepoch 11 | loss: 0.11501 | train_accuracy: 0.9586  | valid_accuracy: 0.95968 |  0:01:49s\nepoch 12 | loss: 0.11314 | train_accuracy: 0.95938 | valid_accuracy: 0.96043 |  0:01:58s\nepoch 13 | loss: 0.11324 | train_accuracy: 0.95789 | valid_accuracy: 0.95915 |  0:02:07s\nepoch 14 | loss: 0.11208 | train_accuracy: 0.95948 | valid_accuracy: 0.96061 |  0:02:17s\nepoch 15 | loss: 0.11157 | train_accuracy: 0.95929 | valid_accuracy: 0.9603  |  0:02:26s\nepoch 16 | loss: 0.1115  | train_accuracy: 0.95938 | valid_accuracy: 0.96003 |  0:02:35s\nepoch 17 | loss: 0.11156 | train_accuracy: 0.95895 | valid_accuracy: 0.96083 |  0:02:43s\nepoch 18 | loss: 0.1116  | train_accuracy: 0.95941 | valid_accuracy: 0.95999 |  0:02:53s\nepoch 19 | loss: 0.111   | train_accuracy: 0.95893 | valid_accuracy: 0.95968 |  0:03:01s\nepoch 20 | loss: 0.11036 | train_accuracy: 0.95957 | valid_accuracy: 0.96128 |  0:03:10s\nepoch 21 | loss: 0.11089 | train_accuracy: 0.95935 | valid_accuracy: 0.96061 |  0:03:19s\nepoch 22 | loss: 0.11053 | train_accuracy: 0.95959 | valid_accuracy: 0.96114 |  0:03:28s\nepoch 23 | loss: 0.11014 | train_accuracy: 0.96006 | valid_accuracy: 0.9603  |  0:03:37s\nepoch 24 | loss: 0.1089  | train_accuracy: 0.96004 | valid_accuracy: 0.96066 |  0:03:46s\nepoch 25 | loss: 0.10938 | train_accuracy: 0.96011 | valid_accuracy: 0.96088 |  0:03:55s\nepoch 26 | loss: 0.1096  | train_accuracy: 0.96038 | valid_accuracy: 0.96119 |  0:04:04s\nepoch 27 | loss: 0.10923 | train_accuracy: 0.96054 | valid_accuracy: 0.96132 |  0:04:13s\nepoch 28 | loss: 0.10902 | train_accuracy: 0.96038 | valid_accuracy: 0.96199 |  0:04:21s\nepoch 29 | loss: 0.10882 | train_accuracy: 0.96011 | valid_accuracy: 0.9615  |  0:04:30s\nepoch 30 | loss: 0.10839 | train_accuracy: 0.96003 | valid_accuracy: 0.96137 |  0:04:39s\nepoch 31 | loss: 0.10878 | train_accuracy: 0.96003 | valid_accuracy: 0.96145 |  0:04:48s\nepoch 32 | loss: 0.1081  | train_accuracy: 0.96013 | valid_accuracy: 0.96172 |  0:04:57s\nepoch 33 | loss: 0.10845 | train_accuracy: 0.96    | valid_accuracy: 0.9611  |  0:05:06s\nepoch 34 | loss: 0.10836 | train_accuracy: 0.96017 | valid_accuracy: 0.96225 |  0:05:16s\nepoch 35 | loss: 0.10788 | train_accuracy: 0.96003 | valid_accuracy: 0.96239 |  0:05:25s\nepoch 36 | loss: 0.10767 | train_accuracy: 0.9605  | valid_accuracy: 0.96137 |  0:05:34s\nepoch 37 | loss: 0.10744 | train_accuracy: 0.95966 | valid_accuracy: 0.96123 |  0:05:42s\nepoch 38 | loss: 0.10775 | train_accuracy: 0.9603  | valid_accuracy: 0.96137 |  0:05:51s\nepoch 39 | loss: 0.10717 | train_accuracy: 0.96082 | valid_accuracy: 0.96168 |  0:06:00s\nepoch 40 | loss: 0.10741 | train_accuracy: 0.96082 | valid_accuracy: 0.9619  |  0:06:09s\nepoch 41 | loss: 0.10737 | train_accuracy: 0.95926 | valid_accuracy: 0.96141 |  0:06:18s\nepoch 42 | loss: 0.10754 | train_accuracy: 0.9608  | valid_accuracy: 0.96221 |  0:06:27s\nepoch 43 | loss: 0.10682 | train_accuracy: 0.96013 | valid_accuracy: 0.96248 |  0:06:36s\nepoch 44 | loss: 0.10688 | train_accuracy: 0.95985 | valid_accuracy: 0.96217 |  0:06:45s\nepoch 45 | loss: 0.107   | train_accuracy: 0.96052 | valid_accuracy: 0.96199 |  0:06:53s\nepoch 46 | loss: 0.10686 | train_accuracy: 0.96092 | valid_accuracy: 0.96159 |  0:07:02s\nepoch 47 | loss: 0.10678 | train_accuracy: 0.96033 | valid_accuracy: 0.96208 |  0:07:11s\nepoch 48 | loss: 0.10677 | train_accuracy: 0.9606  | valid_accuracy: 0.9615  |  0:07:20s\nepoch 49 | loss: 0.1072  | train_accuracy: 0.9612  | valid_accuracy: 0.96181 |  0:07:29s\nepoch 50 | loss: 0.1071  | train_accuracy: 0.96014 | valid_accuracy: 0.96145 |  0:07:38s\nepoch 51 | loss: 0.10727 | train_accuracy: 0.96029 | valid_accuracy: 0.9615  |  0:07:47s\nepoch 52 | loss: 0.10695 | train_accuracy: 0.96008 | valid_accuracy: 0.96181 |  0:07:56s\nepoch 53 | loss: 0.10677 | train_accuracy: 0.96083 | valid_accuracy: 0.96243 |  0:08:04s\nepoch 54 | loss: 0.10703 | train_accuracy: 0.96053 | valid_accuracy: 0.96256 |  0:08:14s\nepoch 55 | loss: 0.10711 | train_accuracy: 0.95999 | valid_accuracy: 0.96168 |  0:08:22s\nepoch 56 | loss: 0.10708 | train_accuracy: 0.9609  | valid_accuracy: 0.96217 |  0:08:31s\nepoch 57 | loss: 0.10749 | train_accuracy: 0.96027 | valid_accuracy: 0.96217 |  0:08:40s\nepoch 58 | loss: 0.10728 | train_accuracy: 0.96102 | valid_accuracy: 0.96252 |  0:08:49s\nepoch 59 | loss: 0.10667 | train_accuracy: 0.96049 | valid_accuracy: 0.96212 |  0:08:58s\nepoch 60 | loss: 0.10721 | train_accuracy: 0.96061 | valid_accuracy: 0.96154 |  0:09:08s\nepoch 61 | loss: 0.1068  | train_accuracy: 0.96081 | valid_accuracy: 0.96199 |  0:09:17s\nepoch 62 | loss: 0.10679 | train_accuracy: 0.96004 | valid_accuracy: 0.96168 |  0:09:26s\nepoch 63 | loss: 0.10668 | train_accuracy: 0.96104 | valid_accuracy: 0.96243 |  0:09:35s\nepoch 64 | loss: 0.10639 | train_accuracy: 0.96103 | valid_accuracy: 0.96265 |  0:09:44s\nepoch 65 | loss: 0.10619 | train_accuracy: 0.96088 | valid_accuracy: 0.96199 |  0:09:53s\nepoch 66 | loss: 0.10601 | train_accuracy: 0.9593  | valid_accuracy: 0.96101 |  0:10:02s\nepoch 67 | loss: 0.1066  | train_accuracy: 0.96074 | valid_accuracy: 0.9619  |  0:10:12s\nepoch 68 | loss: 0.10606 | train_accuracy: 0.96062 | valid_accuracy: 0.96217 |  0:10:21s\nepoch 69 | loss: 0.10654 | train_accuracy: 0.96089 | valid_accuracy: 0.96243 |  0:10:29s\nepoch 70 | loss: 0.10593 | train_accuracy: 0.96104 | valid_accuracy: 0.9623  |  0:10:38s\nepoch 71 | loss: 0.10611 | train_accuracy: 0.9613  | valid_accuracy: 0.96203 |  0:10:47s\nepoch 72 | loss: 0.10579 | train_accuracy: 0.96101 | valid_accuracy: 0.96177 |  0:10:56s\nepoch 73 | loss: 0.10569 | train_accuracy: 0.96063 | valid_accuracy: 0.9615  |  0:11:05s\nepoch 74 | loss: 0.10562 | train_accuracy: 0.96116 | valid_accuracy: 0.96154 |  0:11:14s\nepoch 75 | loss: 0.10583 | train_accuracy: 0.96111 | valid_accuracy: 0.96243 |  0:11:23s\nepoch 76 | loss: 0.10567 | train_accuracy: 0.96118 | valid_accuracy: 0.96159 |  0:11:32s\nepoch 77 | loss: 0.10602 | train_accuracy: 0.96101 | valid_accuracy: 0.96248 |  0:11:41s\nepoch 78 | loss: 0.10576 | train_accuracy: 0.9612  | valid_accuracy: 0.96181 |  0:11:50s\nepoch 79 | loss: 0.10584 | train_accuracy: 0.961   | valid_accuracy: 0.96172 |  0:11:59s\n\nEarly stopping occurred at epoch 79 with best_epoch = 64 and best_valid_accuracy = 0.96265\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Successfully saved model at network.zip.zip\nModel saved to network.zip\nTest Accuracy: 0.9627\nTest F1 Score: 0.9612\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def hyperparameter_tuning(X_train, y_train, X_valid, y_valid):\n    best_acc = 0\n    best_params = {}\n    for n_steps in [3, 5]:\n        for lr in [1e-2, 2e-2, 5e-3]:\n            print(f\"Training with n_steps={n_steps}, lr={lr}\")\n            temp_clf = TabNetClassifier(\n                device_name=device.type,\n                n_d=8,\n                n_a=8,\n                n_steps=n_steps,\n                gamma=1.3,\n                lambda_sparse=1e-3,\n                optimizer_fn=torch.optim.Adam,\n                optimizer_params=dict(lr=lr),\n                scheduler_params={\"step_size\":10, \"gamma\":0.9},\n                scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                mask_type='sparsemax',\n            )\n            temp_clf.fit(\n                X_train, y_train,\n                eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                eval_name=['train', 'valid'],\n                eval_metric=['accuracy'],\n                max_epochs=50,\n                patience=10,\n                batch_size=batch_size,\n                virtual_batch_size=virtual_batch_size,\n                num_workers=0,\n                drop_last=False,\n                \n            )\n            preds_valid = temp_clf.predict(X_valid)\n            acc_valid = accuracy_score(y_valid, preds_valid)\n            print(f\"Validation Accuracy: {acc_valid:.4f}\")\n            if acc_valid > best_acc:\n                best_acc = acc_valid\n                best_params = {\"n_steps\": n_steps, \"lr\": lr}\n    return best_params, best_acc\n\nbest_params, best_acc = hyperparameter_tuning(X_train, y_train, X_test, y_test)\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Validation Accuracy:\", best_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T20:54:02.908283Z","iopub.execute_input":"2025-02-05T20:54:02.908642Z","iopub.status.idle":"2025-02-05T21:18:08.164340Z","shell.execute_reply.started":"2025-02-05T20:54:02.908601Z","shell.execute_reply":"2025-02-05T21:18:08.163663Z"}},"outputs":[{"name":"stdout","text":"Training with n_steps=3, lr=0.01\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n","output_type":"stream"},{"name":"stdout","text":"epoch 0  | loss: 0.20151 | train_accuracy: 0.9528  | valid_accuracy: 0.95346 |  0:00:05s\nepoch 1  | loss: 0.12985 | train_accuracy: 0.95428 | valid_accuracy: 0.95542 |  0:00:10s\nepoch 2  | loss: 0.11879 | train_accuracy: 0.95795 | valid_accuracy: 0.9591  |  0:00:15s\nepoch 3  | loss: 0.11364 | train_accuracy: 0.95879 | valid_accuracy: 0.95994 |  0:00:21s\nepoch 4  | loss: 0.11169 | train_accuracy: 0.95956 | valid_accuracy: 0.96123 |  0:00:26s\nepoch 5  | loss: 0.11073 | train_accuracy: 0.95958 | valid_accuracy: 0.96066 |  0:00:31s\nepoch 6  | loss: 0.11034 | train_accuracy: 0.95982 | valid_accuracy: 0.95999 |  0:00:37s\nepoch 7  | loss: 0.10964 | train_accuracy: 0.96021 | valid_accuracy: 0.96128 |  0:00:42s\nepoch 8  | loss: 0.10917 | train_accuracy: 0.96018 | valid_accuracy: 0.96177 |  0:00:48s\nepoch 9  | loss: 0.10892 | train_accuracy: 0.96042 | valid_accuracy: 0.96119 |  0:00:53s\nepoch 10 | loss: 0.10893 | train_accuracy: 0.96001 | valid_accuracy: 0.96163 |  0:00:58s\nepoch 11 | loss: 0.10851 | train_accuracy: 0.96054 | valid_accuracy: 0.96154 |  0:01:04s\nepoch 12 | loss: 0.10803 | train_accuracy: 0.96024 | valid_accuracy: 0.96132 |  0:01:09s\nepoch 13 | loss: 0.10861 | train_accuracy: 0.96062 | valid_accuracy: 0.96177 |  0:01:14s\nepoch 14 | loss: 0.10827 | train_accuracy: 0.96077 | valid_accuracy: 0.96208 |  0:01:20s\nepoch 15 | loss: 0.1078  | train_accuracy: 0.95912 | valid_accuracy: 0.96043 |  0:01:25s\nepoch 16 | loss: 0.10791 | train_accuracy: 0.95999 | valid_accuracy: 0.96168 |  0:01:31s\nepoch 17 | loss: 0.1076  | train_accuracy: 0.96082 | valid_accuracy: 0.96145 |  0:01:36s\nepoch 18 | loss: 0.10724 | train_accuracy: 0.9601  | valid_accuracy: 0.96101 |  0:01:41s\nepoch 19 | loss: 0.10862 | train_accuracy: 0.96079 | valid_accuracy: 0.96106 |  0:01:47s\nepoch 20 | loss: 0.10731 | train_accuracy: 0.96013 | valid_accuracy: 0.96003 |  0:01:52s\nepoch 21 | loss: 0.10752 | train_accuracy: 0.96076 | valid_accuracy: 0.9619  |  0:01:57s\nepoch 22 | loss: 0.1074  | train_accuracy: 0.96069 | valid_accuracy: 0.96145 |  0:02:02s\nepoch 23 | loss: 0.10677 | train_accuracy: 0.95937 | valid_accuracy: 0.95999 |  0:02:08s\nepoch 24 | loss: 0.10706 | train_accuracy: 0.96091 | valid_accuracy: 0.96181 |  0:02:13s\n\nEarly stopping occurred at epoch 24 with best_epoch = 14 and best_valid_accuracy = 0.96208\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.9621\nTraining with n_steps=3, lr=0.02\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n","output_type":"stream"},{"name":"stdout","text":"epoch 0  | loss: 0.17626 | train_accuracy: 0.95316 | valid_accuracy: 0.95413 |  0:00:05s\nepoch 1  | loss: 0.12546 | train_accuracy: 0.95736 | valid_accuracy: 0.95835 |  0:00:10s\nepoch 2  | loss: 0.11595 | train_accuracy: 0.95967 | valid_accuracy: 0.96021 |  0:00:15s\nepoch 3  | loss: 0.11186 | train_accuracy: 0.95977 | valid_accuracy: 0.96137 |  0:00:21s\nepoch 4  | loss: 0.11016 | train_accuracy: 0.96014 | valid_accuracy: 0.96052 |  0:00:26s\nepoch 5  | loss: 0.10998 | train_accuracy: 0.96048 | valid_accuracy: 0.96092 |  0:00:31s\nepoch 6  | loss: 0.10949 | train_accuracy: 0.96048 | valid_accuracy: 0.96132 |  0:00:36s\nepoch 7  | loss: 0.10829 | train_accuracy: 0.96031 | valid_accuracy: 0.96079 |  0:00:42s\nepoch 8  | loss: 0.10893 | train_accuracy: 0.95978 | valid_accuracy: 0.96097 |  0:00:47s\nepoch 9  | loss: 0.10871 | train_accuracy: 0.95985 | valid_accuracy: 0.96092 |  0:00:52s\nepoch 10 | loss: 0.10823 | train_accuracy: 0.95973 | valid_accuracy: 0.96052 |  0:00:58s\nepoch 11 | loss: 0.10852 | train_accuracy: 0.9608  | valid_accuracy: 0.96185 |  0:01:03s\nepoch 12 | loss: 0.10707 | train_accuracy: 0.96072 | valid_accuracy: 0.9619  |  0:01:08s\nepoch 13 | loss: 0.10745 | train_accuracy: 0.96093 | valid_accuracy: 0.96159 |  0:01:14s\nepoch 14 | loss: 0.1072  | train_accuracy: 0.96052 | valid_accuracy: 0.96163 |  0:01:19s\nepoch 15 | loss: 0.10718 | train_accuracy: 0.96113 | valid_accuracy: 0.96168 |  0:01:24s\nepoch 16 | loss: 0.10693 | train_accuracy: 0.96082 | valid_accuracy: 0.96185 |  0:01:29s\nepoch 17 | loss: 0.10675 | train_accuracy: 0.9612  | valid_accuracy: 0.96194 |  0:01:35s\nepoch 18 | loss: 0.10679 | train_accuracy: 0.95981 | valid_accuracy: 0.96132 |  0:01:40s\nepoch 19 | loss: 0.10704 | train_accuracy: 0.96034 | valid_accuracy: 0.96159 |  0:01:45s\nepoch 20 | loss: 0.1069  | train_accuracy: 0.96129 | valid_accuracy: 0.96181 |  0:01:50s\nepoch 21 | loss: 0.10669 | train_accuracy: 0.96109 | valid_accuracy: 0.96163 |  0:01:56s\nepoch 22 | loss: 0.10712 | train_accuracy: 0.96027 | valid_accuracy: 0.96145 |  0:02:01s\nepoch 23 | loss: 0.10635 | train_accuracy: 0.959   | valid_accuracy: 0.96039 |  0:02:06s\nepoch 24 | loss: 0.10668 | train_accuracy: 0.95975 | valid_accuracy: 0.9603  |  0:02:12s\nepoch 25 | loss: 0.10595 | train_accuracy: 0.96012 | valid_accuracy: 0.96114 |  0:02:17s\nepoch 26 | loss: 0.10686 | train_accuracy: 0.9613  | valid_accuracy: 0.96185 |  0:02:23s\nepoch 27 | loss: 0.1065  | train_accuracy: 0.96062 | valid_accuracy: 0.96172 |  0:02:28s\n\nEarly stopping occurred at epoch 27 with best_epoch = 17 and best_valid_accuracy = 0.96194\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.9619\nTraining with n_steps=3, lr=0.005\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n","output_type":"stream"},{"name":"stdout","text":"epoch 0  | loss: 0.23216 | train_accuracy: 0.94387 | valid_accuracy: 0.94454 |  0:00:05s\nepoch 1  | loss: 0.14852 | train_accuracy: 0.95176 | valid_accuracy: 0.95417 |  0:00:10s\nepoch 2  | loss: 0.1349  | train_accuracy: 0.95449 | valid_accuracy: 0.95577 |  0:00:16s\nepoch 3  | loss: 0.12626 | train_accuracy: 0.95644 | valid_accuracy: 0.95675 |  0:00:21s\nepoch 4  | loss: 0.12032 | train_accuracy: 0.95649 | valid_accuracy: 0.95835 |  0:00:26s\nepoch 5  | loss: 0.11701 | train_accuracy: 0.9583  | valid_accuracy: 0.95937 |  0:00:32s\nepoch 6  | loss: 0.11469 | train_accuracy: 0.95939 | valid_accuracy: 0.95972 |  0:00:37s\nepoch 7  | loss: 0.11194 | train_accuracy: 0.95969 | valid_accuracy: 0.96074 |  0:00:42s\nepoch 8  | loss: 0.11068 | train_accuracy: 0.96023 | valid_accuracy: 0.96097 |  0:00:48s\nepoch 9  | loss: 0.11    | train_accuracy: 0.96042 | valid_accuracy: 0.96132 |  0:00:53s\nepoch 10 | loss: 0.1094  | train_accuracy: 0.96031 | valid_accuracy: 0.96106 |  0:00:59s\nepoch 11 | loss: 0.10936 | train_accuracy: 0.9602  | valid_accuracy: 0.96074 |  0:01:04s\nepoch 12 | loss: 0.10885 | train_accuracy: 0.9609  | valid_accuracy: 0.96128 |  0:01:09s\nepoch 13 | loss: 0.1085  | train_accuracy: 0.96083 | valid_accuracy: 0.9611  |  0:01:15s\nepoch 14 | loss: 0.1089  | train_accuracy: 0.96072 | valid_accuracy: 0.9615  |  0:01:20s\nepoch 15 | loss: 0.1082  | train_accuracy: 0.96081 | valid_accuracy: 0.96106 |  0:01:25s\nepoch 16 | loss: 0.10789 | train_accuracy: 0.96068 | valid_accuracy: 0.96114 |  0:01:31s\nepoch 17 | loss: 0.10752 | train_accuracy: 0.96081 | valid_accuracy: 0.96145 |  0:01:36s\nepoch 18 | loss: 0.10736 | train_accuracy: 0.96048 | valid_accuracy: 0.96088 |  0:01:41s\nepoch 19 | loss: 0.10739 | train_accuracy: 0.96098 | valid_accuracy: 0.96177 |  0:01:46s\nepoch 20 | loss: 0.10674 | train_accuracy: 0.96101 | valid_accuracy: 0.96168 |  0:01:52s\nepoch 21 | loss: 0.10686 | train_accuracy: 0.9612  | valid_accuracy: 0.96163 |  0:01:57s\nepoch 22 | loss: 0.107   | train_accuracy: 0.96132 | valid_accuracy: 0.96128 |  0:02:03s\nepoch 23 | loss: 0.10666 | train_accuracy: 0.96061 | valid_accuracy: 0.96074 |  0:02:08s\nepoch 24 | loss: 0.10671 | train_accuracy: 0.96129 | valid_accuracy: 0.9615  |  0:02:13s\nepoch 25 | loss: 0.10577 | train_accuracy: 0.96149 | valid_accuracy: 0.96177 |  0:02:18s\nepoch 26 | loss: 0.10649 | train_accuracy: 0.96062 | valid_accuracy: 0.9607  |  0:02:24s\nepoch 27 | loss: 0.10635 | train_accuracy: 0.96155 | valid_accuracy: 0.96132 |  0:02:29s\nepoch 28 | loss: 0.10681 | train_accuracy: 0.96122 | valid_accuracy: 0.96163 |  0:02:34s\nepoch 29 | loss: 0.10542 | train_accuracy: 0.96122 | valid_accuracy: 0.96141 |  0:02:40s\n\nEarly stopping occurred at epoch 29 with best_epoch = 19 and best_valid_accuracy = 0.96177\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.9618\nTraining with n_steps=5, lr=0.01\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n","output_type":"stream"},{"name":"stdout","text":"epoch 0  | loss: 0.27158 | train_accuracy: 0.94484 | valid_accuracy: 0.94587 |  0:00:07s\nepoch 1  | loss: 0.14491 | train_accuracy: 0.95032 | valid_accuracy: 0.95169 |  0:00:14s\nepoch 2  | loss: 0.13575 | train_accuracy: 0.95214 | valid_accuracy: 0.95337 |  0:00:22s\nepoch 3  | loss: 0.13113 | train_accuracy: 0.95248 | valid_accuracy: 0.95315 |  0:00:29s\nepoch 4  | loss: 0.12915 | train_accuracy: 0.95378 | valid_accuracy: 0.95368 |  0:00:37s\nepoch 5  | loss: 0.12547 | train_accuracy: 0.95435 | valid_accuracy: 0.95626 |  0:00:44s\nepoch 6  | loss: 0.12353 | train_accuracy: 0.95547 | valid_accuracy: 0.95706 |  0:00:52s\nepoch 7  | loss: 0.11986 | train_accuracy: 0.95599 | valid_accuracy: 0.95764 |  0:00:59s\nepoch 8  | loss: 0.11849 | train_accuracy: 0.95686 | valid_accuracy: 0.95781 |  0:01:07s\nepoch 9  | loss: 0.11948 | train_accuracy: 0.95684 | valid_accuracy: 0.95715 |  0:01:14s\nepoch 10 | loss: 0.11624 | train_accuracy: 0.95655 | valid_accuracy: 0.95732 |  0:01:22s\nepoch 11 | loss: 0.11501 | train_accuracy: 0.9586  | valid_accuracy: 0.95968 |  0:01:29s\nepoch 12 | loss: 0.11314 | train_accuracy: 0.95938 | valid_accuracy: 0.96043 |  0:01:37s\nepoch 13 | loss: 0.11324 | train_accuracy: 0.95789 | valid_accuracy: 0.95915 |  0:01:44s\nepoch 14 | loss: 0.11208 | train_accuracy: 0.95948 | valid_accuracy: 0.96061 |  0:01:52s\nepoch 15 | loss: 0.11157 | train_accuracy: 0.95929 | valid_accuracy: 0.9603  |  0:02:00s\nepoch 16 | loss: 0.1115  | train_accuracy: 0.95938 | valid_accuracy: 0.96003 |  0:02:07s\nepoch 17 | loss: 0.11156 | train_accuracy: 0.95895 | valid_accuracy: 0.96083 |  0:02:14s\nepoch 18 | loss: 0.1116  | train_accuracy: 0.95941 | valid_accuracy: 0.95999 |  0:02:22s\nepoch 19 | loss: 0.111   | train_accuracy: 0.95893 | valid_accuracy: 0.95968 |  0:02:30s\nepoch 20 | loss: 0.11036 | train_accuracy: 0.95957 | valid_accuracy: 0.96128 |  0:02:37s\nepoch 21 | loss: 0.11089 | train_accuracy: 0.95935 | valid_accuracy: 0.96061 |  0:02:44s\nepoch 22 | loss: 0.11053 | train_accuracy: 0.95959 | valid_accuracy: 0.96114 |  0:02:52s\nepoch 23 | loss: 0.11014 | train_accuracy: 0.96006 | valid_accuracy: 0.9603  |  0:02:59s\nepoch 24 | loss: 0.1089  | train_accuracy: 0.96004 | valid_accuracy: 0.96066 |  0:03:07s\nepoch 25 | loss: 0.10938 | train_accuracy: 0.96011 | valid_accuracy: 0.96088 |  0:03:14s\nepoch 26 | loss: 0.1096  | train_accuracy: 0.96038 | valid_accuracy: 0.96119 |  0:03:22s\nepoch 27 | loss: 0.10923 | train_accuracy: 0.96054 | valid_accuracy: 0.96132 |  0:03:29s\nepoch 28 | loss: 0.10902 | train_accuracy: 0.96038 | valid_accuracy: 0.96199 |  0:03:37s\nepoch 29 | loss: 0.10882 | train_accuracy: 0.96011 | valid_accuracy: 0.9615  |  0:03:45s\nepoch 30 | loss: 0.10839 | train_accuracy: 0.96003 | valid_accuracy: 0.96137 |  0:03:52s\nepoch 31 | loss: 0.10878 | train_accuracy: 0.96003 | valid_accuracy: 0.96145 |  0:03:59s\nepoch 32 | loss: 0.1081  | train_accuracy: 0.96013 | valid_accuracy: 0.96172 |  0:04:07s\nepoch 33 | loss: 0.10845 | train_accuracy: 0.96    | valid_accuracy: 0.9611  |  0:04:14s\nepoch 34 | loss: 0.10836 | train_accuracy: 0.96017 | valid_accuracy: 0.96225 |  0:04:22s\nepoch 35 | loss: 0.10788 | train_accuracy: 0.96003 | valid_accuracy: 0.96239 |  0:04:29s\nepoch 36 | loss: 0.10767 | train_accuracy: 0.9605  | valid_accuracy: 0.96137 |  0:04:37s\nepoch 37 | loss: 0.10744 | train_accuracy: 0.95966 | valid_accuracy: 0.96123 |  0:04:44s\nepoch 38 | loss: 0.10775 | train_accuracy: 0.9603  | valid_accuracy: 0.96137 |  0:04:52s\nepoch 39 | loss: 0.10717 | train_accuracy: 0.96082 | valid_accuracy: 0.96168 |  0:04:59s\nepoch 40 | loss: 0.10741 | train_accuracy: 0.96082 | valid_accuracy: 0.9619  |  0:05:07s\nepoch 41 | loss: 0.10737 | train_accuracy: 0.95926 | valid_accuracy: 0.96141 |  0:05:14s\nepoch 42 | loss: 0.10754 | train_accuracy: 0.9608  | valid_accuracy: 0.96221 |  0:05:22s\nepoch 43 | loss: 0.10682 | train_accuracy: 0.96013 | valid_accuracy: 0.96248 |  0:05:29s\nepoch 44 | loss: 0.10688 | train_accuracy: 0.95985 | valid_accuracy: 0.96217 |  0:05:37s\nepoch 45 | loss: 0.107   | train_accuracy: 0.96052 | valid_accuracy: 0.96199 |  0:05:44s\nepoch 46 | loss: 0.10686 | train_accuracy: 0.96092 | valid_accuracy: 0.96159 |  0:05:52s\nepoch 47 | loss: 0.10678 | train_accuracy: 0.96033 | valid_accuracy: 0.96208 |  0:05:59s\nepoch 48 | loss: 0.10677 | train_accuracy: 0.9606  | valid_accuracy: 0.9615  |  0:06:07s\nepoch 49 | loss: 0.1072  | train_accuracy: 0.9612  | valid_accuracy: 0.96181 |  0:06:14s\nStop training because you reached max_epochs = 50 with best_epoch = 43 and best_valid_accuracy = 0.96248\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.9625\nTraining with n_steps=5, lr=0.02\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n","output_type":"stream"},{"name":"stdout","text":"epoch 0  | loss: 0.22126 | train_accuracy: 0.94499 | valid_accuracy: 0.94498 |  0:00:07s\nepoch 1  | loss: 0.13388 | train_accuracy: 0.95365 | valid_accuracy: 0.95391 |  0:00:15s\nepoch 2  | loss: 0.12738 | train_accuracy: 0.95418 | valid_accuracy: 0.95457 |  0:00:22s\nepoch 3  | loss: 0.12802 | train_accuracy: 0.95592 | valid_accuracy: 0.95697 |  0:00:30s\nepoch 4  | loss: 0.12434 | train_accuracy: 0.95492 | valid_accuracy: 0.95479 |  0:00:37s\nepoch 5  | loss: 0.11969 | train_accuracy: 0.95707 | valid_accuracy: 0.95888 |  0:00:45s\nepoch 6  | loss: 0.11604 | train_accuracy: 0.95724 | valid_accuracy: 0.95866 |  0:00:52s\nepoch 7  | loss: 0.11513 | train_accuracy: 0.95755 | valid_accuracy: 0.95981 |  0:01:00s\nepoch 8  | loss: 0.11426 | train_accuracy: 0.95824 | valid_accuracy: 0.96092 |  0:01:07s\nepoch 9  | loss: 0.11449 | train_accuracy: 0.95756 | valid_accuracy: 0.9595  |  0:01:15s\nepoch 10 | loss: 0.11438 | train_accuracy: 0.9574  | valid_accuracy: 0.9587  |  0:01:22s\nepoch 11 | loss: 0.1133  | train_accuracy: 0.959   | valid_accuracy: 0.9611  |  0:01:30s\nepoch 12 | loss: 0.11361 | train_accuracy: 0.95933 | valid_accuracy: 0.96083 |  0:01:37s\nepoch 13 | loss: 0.11337 | train_accuracy: 0.95886 | valid_accuracy: 0.96012 |  0:01:45s\nepoch 14 | loss: 0.11193 | train_accuracy: 0.95832 | valid_accuracy: 0.95883 |  0:01:52s\nepoch 15 | loss: 0.11095 | train_accuracy: 0.95948 | valid_accuracy: 0.9611  |  0:02:00s\nepoch 16 | loss: 0.11096 | train_accuracy: 0.95982 | valid_accuracy: 0.96177 |  0:02:07s\nepoch 17 | loss: 0.11018 | train_accuracy: 0.95899 | valid_accuracy: 0.95963 |  0:02:15s\nepoch 18 | loss: 0.10995 | train_accuracy: 0.95956 | valid_accuracy: 0.9611  |  0:02:22s\nepoch 19 | loss: 0.11079 | train_accuracy: 0.95919 | valid_accuracy: 0.96048 |  0:02:30s\nepoch 20 | loss: 0.11133 | train_accuracy: 0.95992 | valid_accuracy: 0.96185 |  0:02:38s\nepoch 21 | loss: 0.10972 | train_accuracy: 0.96034 | valid_accuracy: 0.96181 |  0:02:45s\nepoch 22 | loss: 0.10915 | train_accuracy: 0.95985 | valid_accuracy: 0.96119 |  0:02:53s\nepoch 23 | loss: 0.10943 | train_accuracy: 0.95994 | valid_accuracy: 0.96137 |  0:03:00s\nepoch 24 | loss: 0.10874 | train_accuracy: 0.96024 | valid_accuracy: 0.96185 |  0:03:08s\nepoch 25 | loss: 0.10898 | train_accuracy: 0.95997 | valid_accuracy: 0.96137 |  0:03:15s\nepoch 26 | loss: 0.10999 | train_accuracy: 0.95967 | valid_accuracy: 0.95999 |  0:03:23s\nepoch 27 | loss: 0.11418 | train_accuracy: 0.95663 | valid_accuracy: 0.95857 |  0:03:30s\nepoch 28 | loss: 0.11211 | train_accuracy: 0.95959 | valid_accuracy: 0.96168 |  0:03:38s\nepoch 29 | loss: 0.11276 | train_accuracy: 0.95943 | valid_accuracy: 0.96021 |  0:03:45s\nepoch 30 | loss: 0.10973 | train_accuracy: 0.95921 | valid_accuracy: 0.96145 |  0:03:53s\n\nEarly stopping occurred at epoch 30 with best_epoch = 20 and best_valid_accuracy = 0.96185\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.9619\nTraining with n_steps=5, lr=0.005\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n","output_type":"stream"},{"name":"stdout","text":"epoch 0  | loss: 0.33604 | train_accuracy: 0.9376  | valid_accuracy: 0.93752 |  0:00:07s\nepoch 1  | loss: 0.16364 | train_accuracy: 0.94579 | valid_accuracy: 0.94507 |  0:00:15s\nepoch 2  | loss: 0.14806 | train_accuracy: 0.94955 | valid_accuracy: 0.95066 |  0:00:23s\nepoch 3  | loss: 0.13823 | train_accuracy: 0.95075 | valid_accuracy: 0.95164 |  0:00:30s\nepoch 4  | loss: 0.13331 | train_accuracy: 0.95214 | valid_accuracy: 0.95213 |  0:00:38s\nepoch 5  | loss: 0.12886 | train_accuracy: 0.95358 | valid_accuracy: 0.95404 |  0:00:45s\nepoch 6  | loss: 0.12781 | train_accuracy: 0.95294 | valid_accuracy: 0.95302 |  0:00:53s\nepoch 7  | loss: 0.12679 | train_accuracy: 0.95444 | valid_accuracy: 0.95599 |  0:01:00s\nepoch 8  | loss: 0.12387 | train_accuracy: 0.95524 | valid_accuracy: 0.95599 |  0:01:08s\nepoch 9  | loss: 0.12183 | train_accuracy: 0.9562  | valid_accuracy: 0.95746 |  0:01:16s\nepoch 10 | loss: 0.12067 | train_accuracy: 0.95532 | valid_accuracy: 0.9551  |  0:01:23s\nepoch 11 | loss: 0.12164 | train_accuracy: 0.95709 | valid_accuracy: 0.95715 |  0:01:31s\nepoch 12 | loss: 0.1197  | train_accuracy: 0.95707 | valid_accuracy: 0.95732 |  0:01:38s\nepoch 13 | loss: 0.11918 | train_accuracy: 0.9574  | valid_accuracy: 0.95768 |  0:01:46s\nepoch 14 | loss: 0.11745 | train_accuracy: 0.95701 | valid_accuracy: 0.95737 |  0:01:53s\nepoch 15 | loss: 0.11806 | train_accuracy: 0.95715 | valid_accuracy: 0.95897 |  0:02:01s\nepoch 16 | loss: 0.11758 | train_accuracy: 0.95742 | valid_accuracy: 0.95937 |  0:02:09s\nepoch 17 | loss: 0.11599 | train_accuracy: 0.95821 | valid_accuracy: 0.95963 |  0:02:16s\nepoch 18 | loss: 0.11508 | train_accuracy: 0.9581  | valid_accuracy: 0.95923 |  0:02:24s\nepoch 19 | loss: 0.11373 | train_accuracy: 0.95818 | valid_accuracy: 0.9591  |  0:02:31s\nepoch 20 | loss: 0.11364 | train_accuracy: 0.95872 | valid_accuracy: 0.96074 |  0:02:39s\nepoch 21 | loss: 0.11312 | train_accuracy: 0.95862 | valid_accuracy: 0.96034 |  0:02:46s\nepoch 22 | loss: 0.11293 | train_accuracy: 0.95852 | valid_accuracy: 0.96012 |  0:02:54s\nepoch 23 | loss: 0.11248 | train_accuracy: 0.95844 | valid_accuracy: 0.96026 |  0:03:01s\nepoch 24 | loss: 0.11214 | train_accuracy: 0.95893 | valid_accuracy: 0.96074 |  0:03:09s\nepoch 25 | loss: 0.11192 | train_accuracy: 0.95926 | valid_accuracy: 0.9607  |  0:03:16s\nepoch 26 | loss: 0.11162 | train_accuracy: 0.95926 | valid_accuracy: 0.96088 |  0:03:24s\nepoch 27 | loss: 0.11094 | train_accuracy: 0.95985 | valid_accuracy: 0.96017 |  0:03:31s\nepoch 28 | loss: 0.11086 | train_accuracy: 0.95973 | valid_accuracy: 0.96101 |  0:03:39s\nepoch 29 | loss: 0.11045 | train_accuracy: 0.9595  | valid_accuracy: 0.96114 |  0:03:47s\nepoch 30 | loss: 0.11019 | train_accuracy: 0.96011 | valid_accuracy: 0.9611  |  0:03:54s\nepoch 31 | loss: 0.11011 | train_accuracy: 0.96002 | valid_accuracy: 0.96074 |  0:04:01s\nepoch 32 | loss: 0.1099  | train_accuracy: 0.95985 | valid_accuracy: 0.96088 |  0:04:09s\nepoch 33 | loss: 0.11032 | train_accuracy: 0.95952 | valid_accuracy: 0.96106 |  0:04:17s\nepoch 34 | loss: 0.10971 | train_accuracy: 0.95997 | valid_accuracy: 0.9607  |  0:04:24s\nepoch 35 | loss: 0.1098  | train_accuracy: 0.9601  | valid_accuracy: 0.96097 |  0:04:32s\nepoch 36 | loss: 0.10933 | train_accuracy: 0.96016 | valid_accuracy: 0.96083 |  0:04:39s\nepoch 37 | loss: 0.11005 | train_accuracy: 0.96041 | valid_accuracy: 0.96141 |  0:04:47s\nepoch 38 | loss: 0.11068 | train_accuracy: 0.95931 | valid_accuracy: 0.9607  |  0:04:54s\nepoch 39 | loss: 0.11013 | train_accuracy: 0.96017 | valid_accuracy: 0.96079 |  0:05:02s\nepoch 40 | loss: 0.10978 | train_accuracy: 0.96023 | valid_accuracy: 0.96074 |  0:05:09s\nepoch 41 | loss: 0.10958 | train_accuracy: 0.95998 | valid_accuracy: 0.96048 |  0:05:17s\nepoch 42 | loss: 0.10965 | train_accuracy: 0.96009 | valid_accuracy: 0.96088 |  0:05:24s\nepoch 43 | loss: 0.10839 | train_accuracy: 0.95997 | valid_accuracy: 0.96052 |  0:05:32s\nepoch 44 | loss: 0.10857 | train_accuracy: 0.96006 | valid_accuracy: 0.96034 |  0:05:39s\nepoch 45 | loss: 0.10948 | train_accuracy: 0.96032 | valid_accuracy: 0.9615  |  0:05:47s\nepoch 46 | loss: 0.10843 | train_accuracy: 0.96028 | valid_accuracy: 0.9607  |  0:05:54s\nepoch 47 | loss: 0.10872 | train_accuracy: 0.95981 | valid_accuracy: 0.9615  |  0:06:02s\nepoch 48 | loss: 0.10914 | train_accuracy: 0.96019 | valid_accuracy: 0.96119 |  0:06:09s\nepoch 49 | loss: 0.1093  | train_accuracy: 0.9606  | valid_accuracy: 0.96114 |  0:06:17s\nStop training because you reached max_epochs = 50 with best_epoch = 45 and best_valid_accuracy = 0.9615\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.9615\nBest Hyperparameters: {'n_steps': 5, 'lr': 0.01}\nBest Validation Accuracy: 0.9624761312669301\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetClassifier\n\ndata_path = \"/kaggle/input/arogoai-assignment/professional_mental.csv\"\ndf = pd.read_csv(data_path)\n\nif \"Unnamed: 0.1\" in df.columns:\n    df.drop([\"Unnamed: 0.1\"], axis=1, inplace=True)\nif \"Unnamed: 0\" in df.columns:\n    df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n\ntarget_col = \"Depression\"\nX = df.drop(target_col, axis=1).values\ny = df[target_col].values\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nclf = TabNetClassifier(\n    input_dim=13,\n    device_name=device.type,\n    \n    n_d=8,              \n    n_a=8,              \n    n_steps=5,          \n    gamma=1.3,           \n    lambda_sparse=1e-3, \n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=0.01),\n    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n    mask_type='sparsemax', \n)\n\nmax_epochs = 100\nbatch_size = 1024\nvirtual_batch_size = 128\n\nclf.fit(\n    X, y,\n    max_epochs=max_epochs,\n    patience=15,  \n    batch_size=batch_size,\n    virtual_batch_size=virtual_batch_size,\n    num_workers=0,\n    drop_last=False\n)\n\nmodel_path = \"network.zip\"\nclf.save_model(model_path)\nprint(f\"Model saved to {model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T10:47:07.476655Z","iopub.execute_input":"2025-02-07T10:47:07.476988Z","iopub.status.idle":"2025-02-07T10:56:38.146856Z","shell.execute_reply.started":"2025-02-07T10:47:07.476961Z","shell.execute_reply":"2025-02-07T10:56:38.145934Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n  warnings.warn(wrn_msg)\n","output_type":"stream"},{"name":"stdout","text":"epoch 0  | loss: 0.24194 |  0:00:05s\nepoch 1  | loss: 0.15228 |  0:00:11s\nepoch 2  | loss: 0.14162 |  0:00:17s\nepoch 3  | loss: 0.13267 |  0:00:22s\nepoch 4  | loss: 0.12625 |  0:00:28s\nepoch 5  | loss: 0.12454 |  0:00:33s\nepoch 6  | loss: 0.12252 |  0:00:39s\nepoch 7  | loss: 0.12105 |  0:00:45s\nepoch 8  | loss: 0.12089 |  0:00:51s\nepoch 9  | loss: 0.11925 |  0:00:56s\nepoch 10 | loss: 0.11751 |  0:01:02s\nepoch 11 | loss: 0.11568 |  0:01:07s\nepoch 12 | loss: 0.11417 |  0:01:13s\nepoch 13 | loss: 0.11179 |  0:01:19s\nepoch 14 | loss: 0.11134 |  0:01:25s\nepoch 15 | loss: 0.11008 |  0:01:30s\nepoch 16 | loss: 0.10946 |  0:01:36s\nepoch 17 | loss: 0.10875 |  0:01:41s\nepoch 18 | loss: 0.10856 |  0:01:47s\nepoch 19 | loss: 0.10801 |  0:01:53s\nepoch 20 | loss: 0.10766 |  0:01:59s\nepoch 21 | loss: 0.10759 |  0:02:04s\nepoch 22 | loss: 0.10723 |  0:02:10s\nepoch 23 | loss: 0.10797 |  0:02:16s\nepoch 24 | loss: 0.10733 |  0:02:21s\nepoch 25 | loss: 0.10803 |  0:02:27s\nepoch 26 | loss: 0.10701 |  0:02:33s\nepoch 27 | loss: 0.107   |  0:02:38s\nepoch 28 | loss: 0.10656 |  0:02:44s\nepoch 29 | loss: 0.10897 |  0:02:50s\nepoch 30 | loss: 0.10953 |  0:02:55s\nepoch 31 | loss: 0.10855 |  0:03:01s\nepoch 32 | loss: 0.10799 |  0:03:06s\nepoch 33 | loss: 0.10832 |  0:03:12s\nepoch 34 | loss: 0.10814 |  0:03:18s\nepoch 35 | loss: 0.10817 |  0:03:24s\nepoch 36 | loss: 0.10898 |  0:03:29s\nepoch 37 | loss: 0.10803 |  0:03:35s\nepoch 38 | loss: 0.10816 |  0:03:41s\nepoch 39 | loss: 0.10866 |  0:03:46s\nepoch 40 | loss: 0.10729 |  0:03:52s\nepoch 41 | loss: 0.10681 |  0:03:57s\nepoch 42 | loss: 0.10694 |  0:04:03s\nepoch 43 | loss: 0.10653 |  0:04:09s\nepoch 44 | loss: 0.10634 |  0:04:14s\nepoch 45 | loss: 0.10656 |  0:04:20s\nepoch 46 | loss: 0.106   |  0:04:26s\nepoch 47 | loss: 0.10663 |  0:04:31s\nepoch 48 | loss: 0.10672 |  0:04:37s\nepoch 49 | loss: 0.1075  |  0:04:43s\nepoch 50 | loss: 0.10658 |  0:04:48s\nepoch 51 | loss: 0.10573 |  0:04:54s\nepoch 52 | loss: 0.10605 |  0:05:00s\nepoch 53 | loss: 0.10593 |  0:05:05s\nepoch 54 | loss: 0.10575 |  0:05:11s\nepoch 55 | loss: 0.10593 |  0:05:17s\nepoch 56 | loss: 0.10725 |  0:05:22s\nepoch 57 | loss: 0.10686 |  0:05:28s\nepoch 58 | loss: 0.10805 |  0:05:34s\nepoch 59 | loss: 0.10689 |  0:05:39s\nepoch 60 | loss: 0.10855 |  0:05:45s\nepoch 61 | loss: 0.10886 |  0:05:50s\nepoch 62 | loss: 0.10851 |  0:05:56s\nepoch 63 | loss: 0.1075  |  0:06:02s\nepoch 64 | loss: 0.10693 |  0:06:08s\nepoch 65 | loss: 0.10606 |  0:06:13s\nepoch 66 | loss: 0.10632 |  0:06:19s\nepoch 67 | loss: 0.10643 |  0:06:24s\nepoch 68 | loss: 0.10575 |  0:06:30s\nepoch 69 | loss: 0.10584 |  0:06:36s\nepoch 70 | loss: 0.10584 |  0:06:42s\nepoch 71 | loss: 0.10576 |  0:06:47s\nepoch 72 | loss: 0.10543 |  0:06:53s\nepoch 73 | loss: 0.10555 |  0:06:59s\nepoch 74 | loss: 0.1056  |  0:07:04s\nepoch 75 | loss: 0.10578 |  0:07:10s\nepoch 76 | loss: 0.10532 |  0:07:16s\nepoch 77 | loss: 0.10563 |  0:07:21s\nepoch 78 | loss: 0.10572 |  0:07:27s\nepoch 79 | loss: 0.10637 |  0:07:33s\nepoch 80 | loss: 0.10603 |  0:07:38s\nepoch 81 | loss: 0.10543 |  0:07:44s\nepoch 82 | loss: 0.10567 |  0:07:50s\nepoch 83 | loss: 0.10543 |  0:07:55s\nepoch 84 | loss: 0.10518 |  0:08:01s\nepoch 85 | loss: 0.10692 |  0:08:07s\nepoch 86 | loss: 0.10654 |  0:08:13s\nepoch 87 | loss: 0.1056  |  0:08:18s\nepoch 88 | loss: 0.10579 |  0:08:24s\nepoch 89 | loss: 0.106   |  0:08:30s\nepoch 90 | loss: 0.1076  |  0:08:35s\nepoch 91 | loss: 0.10594 |  0:08:41s\nepoch 92 | loss: 0.10549 |  0:08:47s\nepoch 93 | loss: 0.10524 |  0:08:52s\nepoch 94 | loss: 0.10527 |  0:08:58s\nepoch 95 | loss: 0.10553 |  0:09:03s\nepoch 96 | loss: 0.10535 |  0:09:09s\nepoch 97 | loss: 0.1052  |  0:09:15s\nepoch 98 | loss: 0.10559 |  0:09:21s\nepoch 99 | loss: 0.10502 |  0:09:26s\nSuccessfully saved model at network.zip.zip\nModel saved to network.zip\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# TabNet on student dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetClassifier\n\ndata_path = \"/kaggle/input/mental-health-arogoai/student_mental.csv\"\ndf = pd.read_csv(data_path)\n\n\nif \"Unnamed: 0.1\" in df.columns:\n    df.drop([\"Unnamed: 0.1\"], axis=1, inplace=True)\nif \"Unnamed: 0\" in df.columns:\n    df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n\ntarget_col = \"Depression\"\nX = df.drop(target_col, axis=1).values\ny = df[target_col].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nclf = TabNetClassifier(\n    device_name=device.type,\n    n_d=8,              \n    n_a=8,              \n    n_steps=5,          \n    gamma=1.3,           \n    lambda_sparse=1e-3, \n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=0.01),\n    scheduler_params={\"step_size\":10, \"gamma\":0.9},\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n    mask_type='sparsemax', \n)\n\nmax_epochs = 100\nbatch_size = 1024\nvirtual_batch_size = 128\n\nclf.fit(\n    X_train, y_train,\n    eval_set=[(X_train, y_train), (X_test, y_test)],\n    eval_name=['train', 'valid'],\n    eval_metric=['accuracy'],\n    max_epochs=max_epochs,\n    patience=15,  \n    batch_size=batch_size,\n    virtual_batch_size=virtual_batch_size,\n    num_workers=0,\n    drop_last=False\n)\n\nmodel_path = \"tabnet_model.zip\"\nclf.save_model(model_path)\nprint(f\"Model saved to {model_path}\")\n\npreds = clf.predict(X_test)\npred_probs = clf.predict_proba(X_test)\n\nacc = accuracy_score(y_test, preds)\nf1 = f1_score(y_test, preds, average=\"weighted\")  \nprint(f\"Test Accuracy: {acc:.4f}\")\nprint(f\"Test F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:13:00.245620Z","iopub.execute_input":"2025-02-06T13:13:00.246162Z","iopub.status.idle":"2025-02-06T13:14:31.340164Z","shell.execute_reply.started":"2025-02-06T13:13:00.246131Z","shell.execute_reply":"2025-02-06T13:14:31.339473Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n  warnings.warn(f\"Device used : {self.device}\")\n","output_type":"stream"},{"name":"stdout","text":"epoch 0  | loss: 0.72072 | train_accuracy: 0.74182 | valid_accuracy: 0.75265 |  0:00:02s\nepoch 1  | loss: 0.50392 | train_accuracy: 0.78866 | valid_accuracy: 0.80011 |  0:00:04s\nepoch 2  | loss: 0.47328 | train_accuracy: 0.79382 | valid_accuracy: 0.79975 |  0:00:06s\nepoch 3  | loss: 0.45391 | train_accuracy: 0.81117 | valid_accuracy: 0.81521 |  0:00:08s\nepoch 4  | loss: 0.42666 | train_accuracy: 0.81643 | valid_accuracy: 0.82438 |  0:00:10s\nepoch 5  | loss: 0.41529 | train_accuracy: 0.8203  | valid_accuracy: 0.82491 |  0:00:12s\nepoch 6  | loss: 0.40502 | train_accuracy: 0.82605 | valid_accuracy: 0.83031 |  0:00:14s\nepoch 7  | loss: 0.39603 | train_accuracy: 0.82583 | valid_accuracy: 0.83534 |  0:00:16s\nepoch 8  | loss: 0.39156 | train_accuracy: 0.83005 | valid_accuracy: 0.83876 |  0:00:17s\nepoch 9  | loss: 0.38867 | train_accuracy: 0.83634 | valid_accuracy: 0.83876 |  0:00:20s\nepoch 10 | loss: 0.38105 | train_accuracy: 0.83724 | valid_accuracy: 0.84145 |  0:00:21s\nepoch 11 | loss: 0.37818 | train_accuracy: 0.83657 | valid_accuracy: 0.84756 |  0:00:23s\nepoch 12 | loss: 0.3746  | train_accuracy: 0.83652 | valid_accuracy: 0.84253 |  0:00:25s\nepoch 13 | loss: 0.37535 | train_accuracy: 0.83904 | valid_accuracy: 0.84541 |  0:00:27s\nepoch 14 | loss: 0.37456 | train_accuracy: 0.8412  | valid_accuracy: 0.84792 |  0:00:29s\nepoch 15 | loss: 0.37397 | train_accuracy: 0.83886 | valid_accuracy: 0.84505 |  0:00:31s\nepoch 16 | loss: 0.37164 | train_accuracy: 0.83963 | valid_accuracy: 0.84505 |  0:00:33s\nepoch 17 | loss: 0.37198 | train_accuracy: 0.83945 | valid_accuracy: 0.84649 |  0:00:35s\nepoch 18 | loss: 0.37271 | train_accuracy: 0.84057 | valid_accuracy: 0.84613 |  0:00:36s\nepoch 19 | loss: 0.3695  | train_accuracy: 0.84142 | valid_accuracy: 0.84685 |  0:00:38s\nepoch 20 | loss: 0.36746 | train_accuracy: 0.84228 | valid_accuracy: 0.84774 |  0:00:40s\nepoch 21 | loss: 0.36813 | train_accuracy: 0.84165 | valid_accuracy: 0.84828 |  0:00:42s\nepoch 22 | loss: 0.36895 | train_accuracy: 0.84241 | valid_accuracy: 0.84613 |  0:00:44s\nepoch 23 | loss: 0.36541 | train_accuracy: 0.84277 | valid_accuracy: 0.8481  |  0:00:46s\nepoch 24 | loss: 0.36646 | train_accuracy: 0.84111 | valid_accuracy: 0.84846 |  0:00:48s\nepoch 25 | loss: 0.36671 | train_accuracy: 0.84268 | valid_accuracy: 0.84882 |  0:00:50s\nepoch 26 | loss: 0.36625 | train_accuracy: 0.8407  | valid_accuracy: 0.84685 |  0:00:52s\nepoch 27 | loss: 0.36784 | train_accuracy: 0.84228 | valid_accuracy: 0.85062 |  0:00:53s\nepoch 28 | loss: 0.36395 | train_accuracy: 0.84457 | valid_accuracy: 0.84828 |  0:00:55s\nepoch 29 | loss: 0.36535 | train_accuracy: 0.84246 | valid_accuracy: 0.85026 |  0:00:57s\nepoch 30 | loss: 0.36526 | train_accuracy: 0.84417 | valid_accuracy: 0.8508  |  0:00:59s\nepoch 31 | loss: 0.36237 | train_accuracy: 0.84219 | valid_accuracy: 0.84667 |  0:01:01s\nepoch 32 | loss: 0.36788 | train_accuracy: 0.84097 | valid_accuracy: 0.84918 |  0:01:03s\nepoch 33 | loss: 0.36584 | train_accuracy: 0.84264 | valid_accuracy: 0.849   |  0:01:05s\nepoch 34 | loss: 0.36819 | train_accuracy: 0.84111 | valid_accuracy: 0.84361 |  0:01:07s\nepoch 35 | loss: 0.36656 | train_accuracy: 0.84147 | valid_accuracy: 0.84738 |  0:01:09s\nepoch 36 | loss: 0.36786 | train_accuracy: 0.84088 | valid_accuracy: 0.84828 |  0:01:10s\nepoch 37 | loss: 0.36564 | train_accuracy: 0.84192 | valid_accuracy: 0.84882 |  0:01:12s\nepoch 38 | loss: 0.36581 | train_accuracy: 0.8425  | valid_accuracy: 0.84667 |  0:01:14s\nepoch 39 | loss: 0.36546 | train_accuracy: 0.84426 | valid_accuracy: 0.84523 |  0:01:16s\nepoch 40 | loss: 0.36559 | train_accuracy: 0.84453 | valid_accuracy: 0.85008 |  0:01:18s\nepoch 41 | loss: 0.36449 | train_accuracy: 0.84569 | valid_accuracy: 0.84702 |  0:01:20s\nepoch 42 | loss: 0.36357 | train_accuracy: 0.84605 | valid_accuracy: 0.84667 |  0:01:22s\nepoch 43 | loss: 0.36394 | train_accuracy: 0.84542 | valid_accuracy: 0.84936 |  0:01:24s\nepoch 44 | loss: 0.36391 | train_accuracy: 0.84569 | valid_accuracy: 0.84595 |  0:01:26s\nepoch 45 | loss: 0.36372 | train_accuracy: 0.84578 | valid_accuracy: 0.84649 |  0:01:27s\n\nEarly stopping occurred at epoch 45 with best_epoch = 30 and best_valid_accuracy = 0.8508\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Successfully saved model at tabnet_model.zip.zip\nModel saved to tabnet_model.zip\nTest Accuracy: 0.8508\nTest F1 Score: 0.8501\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n\ndf = pd.read_csv(\"/kaggle/input/mental-health-arogoai/student_mental.csv\")\ndf.drop(columns=['Unnamed: 0'] , inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:21:53.142373Z","iopub.execute_input":"2025-02-06T13:21:53.142699Z","iopub.status.idle":"2025-02-06T13:21:53.181163Z","shell.execute_reply.started":"2025-02-06T13:21:53.142675Z","shell.execute_reply":"2025-02-06T13:21:53.180153Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport logging\nlogging.getLogger(\"xgboost\").setLevel(logging.CRITICAL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:31:58.788129Z","iopub.execute_input":"2025-02-06T13:31:58.788441Z","iopub.status.idle":"2025-02-06T13:31:58.792507Z","shell.execute_reply.started":"2025-02-06T13:31:58.788417Z","shell.execute_reply":"2025-02-06T13:31:58.791397Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"X = df.drop(\"Depression\", axis=1)\ny = df[\"Depression\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nspace = {\n    'max_depth': hp.quniform('max_depth', 3, 12, 1),\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n    'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),\n    'gamma': hp.uniform('gamma', 0, 0.5),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n    'subsample': hp.uniform('subsample', 0.5, 1),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)\n}\n\ndef objective(params):\n    params['max_depth'] = int(params['max_depth'])\n    params['n_estimators'] = int(params['n_estimators'])\n    params['min_child_weight'] = int(params['min_child_weight'])\n    model = XGBClassifier(\n        booster='gbtree',\n        objective='binary:logistic',\n        use_label_encoder=False,\n        eval_metric='logloss',\n        tree_method='gpu_hist',\n        predictor='gpu_predictor',\n        **params\n    )\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    acc = accuracy_score(y_test, preds)\n    return {'loss': -acc, 'status': STATUS_OK}\n\ntrials = Trials()\nbest = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=np.random.default_rng(42))\nbest['max_depth'] = int(best['max_depth'])\nbest['n_estimators'] = int(best['n_estimators'])\nbest['min_child_weight'] = int(best['min_child_weight'])\n\nfinal_model = XGBClassifier(\n    booster='gbtree',\n    objective='binary:logistic',\n    use_label_encoder=False,\n    eval_metric='logloss',\n    tree_method='gpu_hist',\n    predictor='gpu_predictor',\n    **best\n)\nfinal_model.fit(X_train, y_train)\npreds = final_model.predict(X_test)\nprint(\"Final Accuracy:\", accuracy_score(y_test, preds))\nprint(\"Best Hyperparameters:\", best)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:32:04.454230Z","iopub.execute_input":"2025-02-06T13:32:04.454541Z","iopub.status.idle":"2025-02-06T13:32:52.642748Z","shell.execute_reply.started":"2025-02-06T13:32:04.454515Z","shell.execute_reply":"2025-02-06T13:32:52.641998Z"}},"outputs":[{"name":"stdout","text":"100%|██████████| 50/50 [00:47<00:00,  1.04trial/s, best loss: -0.8565522200251663]\nFinal Accuracy: 0.8565522200251663\nBest Hyperparameters: {'colsample_bytree': 0.5755485717971284, 'gamma': 0.02218720951511316, 'learning_rate': 0.16808878278453498, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 184, 'subsample': 0.5960466731264948}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import pickle\npickle.dump(final_model, open(\"xgboost_model.pkl\", \"wb\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:35:55.463764Z","iopub.execute_input":"2025-02-06T14:35:55.464086Z","iopub.status.idle":"2025-02-06T14:35:55.471767Z","shell.execute_reply.started":"2025-02-06T14:35:55.464060Z","shell.execute_reply":"2025-02-06T14:35:55.471049Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Random forest for student datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:57:58.789052Z","iopub.execute_input":"2025-02-06T14:57:58.789373Z","iopub.status.idle":"2025-02-06T14:57:59.074897Z","shell.execute_reply.started":"2025-02-06T14:57:58.789350Z","shell.execute_reply":"2025-02-06T14:57:59.074275Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nrf_model = RandomForestClassifier(random_state=42)\n\nparam_grid = {\n    'n_estimators': [500, 700, 900],\n    'max_depth': [3, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='f1')\ngrid_search.fit(X_train, y_train)\n\nbest_rf_model = grid_search.best_estimator_\n\ny_pred = best_rf_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average='binary')\n\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"F1 Score: {f1:.2f}\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nwith open(\"random_forest_model.pkl\", \"wb\") as file:\n    pickle.dump(best_rf_model, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:58:42.296898Z","iopub.execute_input":"2025-02-06T14:58:42.297254Z","iopub.status.idle":"2025-02-06T15:07:45.430207Z","shell.execute_reply.started":"2025-02-06T14:58:42.297229Z","shell.execute_reply":"2025-02-06T15:07:45.429128Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 84.24%\nF1 Score: 0.87\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.83      0.77      0.80      2293\n           1       0.85      0.89      0.87      3270\n\n    accuracy                           0.84      5563\n   macro avg       0.84      0.83      0.84      5563\nweighted avg       0.84      0.84      0.84      5563\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Tabtransformer for student dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n\nclass TabTransformer(nn.Module):\n    def __init__(self, num_features, embed_dim, depth, num_heads, dropout, mlp_hidden_dim, num_classes):\n        super(TabTransformer, self).__init__()\n        self.feature_projection = nn.Linear(1, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n        self.mlp = nn.Sequential(\n            nn.Linear(num_features * embed_dim, mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_hidden_dim, num_classes)\n        )\n    def forward(self, x):\n        x = x.unsqueeze(-1)\n        x = self.feature_projection(x)\n        x = self.transformer(x)\n        x = x.flatten(1)\n        return self.mlp(x)\n\ndef train_model(model, optimizer, criterion, loader, device):\n    model.train()\n    total_loss = 0\n    for xb, yb in loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        outputs = model(xb)\n        loss = criterion(outputs, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * xb.size(0)\n    return total_loss / len(loader.dataset)\n\ndef evaluate_model(model, loader, device):\n    model.eval()\n    preds, targets = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            outputs = model(xb)\n            preds.append(torch.argmax(outputs, dim=1).cpu().numpy())\n            targets.append(yb.cpu().numpy())\n    preds = np.concatenate(preds)\n    targets = np.concatenate(targets)\n    return accuracy_score(targets, preds), f1_score(targets, preds, average='weighted')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:09:24.597650Z","iopub.execute_input":"2025-02-06T14:09:24.597948Z","iopub.status.idle":"2025-02-06T14:09:24.607718Z","shell.execute_reply.started":"2025-02-06T14:09:24.597924Z","shell.execute_reply":"2025-02-06T14:09:24.606883Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\ndf = pd.read_csv(\"/kaggle/input/mental-health-arogoai/student_mental.csv\")\nX = df.drop(\"Depression\", axis=1).values\ny = df[\"Depression\"].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:11:12.351276Z","iopub.execute_input":"2025-02-06T14:11:12.351599Z","iopub.status.idle":"2025-02-06T14:11:12.384457Z","shell.execute_reply.started":"2025-02-06T14:11:12.351572Z","shell.execute_reply":"2025-02-06T14:11:12.383817Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\nbatch_size = 32\ntrain_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)), batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)), batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)), batch_size=batch_size, shuffle=False)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef objective(params):\n    embed_dim = int(params['embed_dim'])\n    depth = int(params['depth'])\n    num_heads = int(params['num_heads'])\n    dropout = params['dropout']\n    mlp_hidden_dim = int(params['mlp_hidden_dim'])\n    lr = params['lr']\n    if embed_dim % num_heads != 0:\n        return {'loss': 1.0, 'status': STATUS_OK}\n    model = TabTransformer(num_features=X_train.shape[1], embed_dim=embed_dim, depth=depth, num_heads=num_heads, dropout=dropout, mlp_hidden_dim=mlp_hidden_dim, num_classes=2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    epochs = 10\n    for _ in range(epochs):\n        train_model(model, optimizer, criterion, train_loader, device)\n    acc, _ = evaluate_model(model, val_loader, device)\n    return {'loss': -acc, 'status': STATUS_OK}\n\nspace = {\n    'embed_dim': hp.quniform('embed_dim', 16, 64, 8),\n    'depth': hp.quniform('depth', 1, 4, 1),\n    'num_heads': hp.quniform('num_heads', 1, 4, 1),\n    'dropout': hp.uniform('dropout', 0.1, 0.5),\n    'mlp_hidden_dim': hp.quniform('mlp_hidden_dim', 32, 256, 32),\n    'lr': hp.loguniform('lr', np.log(1e-4), np.log(1e-2))\n}\n\ntrials = Trials()\nbest = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20, trials=trials, rstate=np.random.default_rng(42))\nbest['embed_dim'] = int(best['embed_dim'])\nbest['depth'] = int(best['depth'])\nbest['num_heads'] = int(best['num_heads'])\nbest['mlp_hidden_dim'] = int(best['mlp_hidden_dim'])\n\nmodel = TabTransformer(num_features=X_train.shape[1], embed_dim=best['embed_dim'], depth=best['depth'], num_heads=best['num_heads'], dropout=best['dropout'], mlp_hidden_dim=best['mlp_hidden_dim'], num_classes=2).to(device)\noptimizer = optim.Adam(model.parameters(), lr=best['lr'])\ncriterion = nn.CrossEntropyLoss()\nepochs = 25\nfor _ in range(epochs):\n    train_model(model, optimizer, criterion, train_loader, device)\nval_acc, val_f1 = evaluate_model(model, val_loader, device)\ntest_acc, test_f1 = evaluate_model(model, test_loader, device)\nprint(\"Validation Accuracy:\", val_acc, \"Validation F1:\", val_f1)\nprint(\"Test Accuracy:\", test_acc, \"Test F1:\", test_f1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:11:13.274748Z","iopub.execute_input":"2025-02-06T14:11:13.275095Z","iopub.status.idle":"2025-02-06T14:23:51.627562Z","shell.execute_reply.started":"2025-02-06T14:11:13.275067Z","shell.execute_reply":"2025-02-06T14:23:51.626695Z"}},"outputs":[{"name":"stdout","text":"100%|██████████| 20/20 [10:37<00:00, 31.86s/trial, best loss: -0.8542665388302972]\nValidation Accuracy: 0.8511505273250239 Validation F1: 0.8506410884160949\nTest Accuracy: 0.8487535953978907 Test F1: 0.8482669857844034\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}