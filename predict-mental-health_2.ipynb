{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10663249,"sourceType":"datasetVersion","datasetId":6603848}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-05T03:56:21.835189Z","iopub.execute_input":"2025-02-05T03:56:21.835532Z","iopub.status.idle":"2025-02-05T03:56:21.844199Z","shell.execute_reply.started":"2025-02-05T03:56:21.835500Z","shell.execute_reply":"2025-02-05T03:56:21.843471Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/arogo-final/final_99999.csv\n/kaggle/input/arogo-final/final_median.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\ndf2 = pd.read_csv(\"/kaggle/input/arogo-final/final_median.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T03:56:25.234046Z","iopub.execute_input":"2025-02-05T03:56:25.234482Z","iopub.status.idle":"2025-02-05T03:56:25.849149Z","shell.execute_reply.started":"2025-02-05T03:56:25.234444Z","shell.execute_reply":"2025-02-05T03:56:25.848116Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df2 = df2.drop(columns=['Unnamed: 0'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T03:56:27.663709Z","iopub.execute_input":"2025-02-05T03:56:27.664045Z","iopub.status.idle":"2025-02-05T03:56:27.683072Z","shell.execute_reply.started":"2025-02-05T03:56:27.664018Z","shell.execute_reply":"2025-02-05T03:56:27.681736Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df2.sample(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T03:56:28.942329Z","iopub.execute_input":"2025-02-05T03:56:28.942714Z","iopub.status.idle":"2025-02-05T03:56:28.973515Z","shell.execute_reply.started":"2025-02-05T03:56:28.942675Z","shell.execute_reply":"2025-02-05T03:56:28.972620Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        Gender   Age  City  Working Professional or Student  Profession  \\\n122780       0  27.0     5                                0           1   \n61616        0  53.0     9                                1          10   \n90911        0  49.0    12                                1          12   \n123344       1  27.0    20                                1           4   \n117784       1  48.0    18                                1           7   \n\n        Academic Pressure  Work Pressure  CGPA  Study Satisfaction  \\\n122780                1.0            3.0  5.69                 5.0   \n61616                 3.0            2.0  7.77                 3.0   \n90911                 3.0            5.0  7.77                 3.0   \n123344                3.0            5.0  7.77                 3.0   \n117784                3.0            4.0  7.77                 3.0   \n\n        Job Satisfaction  Sleep Duration  Dietary Habits  Degree  \\\n122780               3.0               3               2      16   \n61616                5.0               4               1      20   \n90911                2.0               4               3      13   \n123344               1.0               3               1       2   \n117784               2.0               2               3      14   \n\n        Have you ever had suicidal thoughts ?  Work/Study Hours  \\\n122780                                      0               5.0   \n61616                                       1               1.0   \n90911                                       0               6.0   \n123344                                      1               5.0   \n117784                                      0               0.0   \n\n        Financial Stress  Family History of Mental Illness  Depression  \n122780               2.0                                 0           0  \n61616                5.0                                 0           0  \n90911                4.0                                 1           0  \n123344               5.0                                 1           1  \n117784               2.0                                 0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>City</th>\n      <th>Working Professional or Student</th>\n      <th>Profession</th>\n      <th>Academic Pressure</th>\n      <th>Work Pressure</th>\n      <th>CGPA</th>\n      <th>Study Satisfaction</th>\n      <th>Job Satisfaction</th>\n      <th>Sleep Duration</th>\n      <th>Dietary Habits</th>\n      <th>Degree</th>\n      <th>Have you ever had suicidal thoughts ?</th>\n      <th>Work/Study Hours</th>\n      <th>Financial Stress</th>\n      <th>Family History of Mental Illness</th>\n      <th>Depression</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>122780</th>\n      <td>0</td>\n      <td>27.0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>5.69</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>16</td>\n      <td>0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61616</th>\n      <td>0</td>\n      <td>53.0</td>\n      <td>9</td>\n      <td>1</td>\n      <td>10</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>7.77</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>20</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>90911</th>\n      <td>0</td>\n      <td>49.0</td>\n      <td>12</td>\n      <td>1</td>\n      <td>12</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>7.77</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>13</td>\n      <td>0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>123344</th>\n      <td>1</td>\n      <td>27.0</td>\n      <td>20</td>\n      <td>1</td>\n      <td>4</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>7.77</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>117784</th>\n      <td>1</td>\n      <td>48.0</td>\n      <td>18</td>\n      <td>1</td>\n      <td>7</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>7.77</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"X = df2.drop('Depression', axis=1)\ny = df2['Depression']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T03:56:32.911132Z","iopub.execute_input":"2025-02-05T03:56:32.911443Z","iopub.status.idle":"2025-02-05T03:56:32.919733Z","shell.execute_reply.started":"2025-02-05T03:56:32.911419Z","shell.execute_reply":"2025-02-05T03:56:32.918587Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ANN Implementation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test).flatten()\nclasses = np.unique(y_train)\nclass_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\nclass_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n\nmodel = Sequential([\n    Input(shape=(X_train.shape[1],)),\n    Dense(256, activation='elu', kernel_regularizer=l2(0.001)),\n    BatchNormalization(),\n    \n    Dense(128, activation='elu', kernel_regularizer=l2(0.001)),\n    BatchNormalization(),\n    Dropout(0.4),\n    Dense(64, activation='elu', kernel_regularizer=l2(0.001)),\n    BatchNormalization(),\n    Dropout(0.3),\n    Dense(32, activation='elu'),\n    BatchNormalization(),\n    Dropout(0.3),\n    Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=[early_stop, lr_scheduler], class_weight=class_weight_dict)\nloss, acc = model.evaluate(X_test, y_test)\nprint(\"Test Accuracy:\", acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T18:30:40.394544Z","iopub.execute_input":"2025-02-04T18:30:40.394866Z","iopub.status.idle":"2025-02-04T18:31:40.343823Z","shell.execute_reply.started":"2025-02-04T18:30:40.394838Z","shell.execute_reply":"2025-02-04T18:31:40.343137Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8483 - loss: 0.5748 - val_accuracy: 0.9100 - val_loss: 0.3851 - learning_rate: 5.0000e-04\nEpoch 2/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9034 - loss: 0.3634 - val_accuracy: 0.9043 - val_loss: 0.2988 - learning_rate: 5.0000e-04\nEpoch 3/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9029 - loss: 0.2868 - val_accuracy: 0.9058 - val_loss: 0.2545 - learning_rate: 5.0000e-04\nEpoch 4/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9036 - loss: 0.2529 - val_accuracy: 0.9104 - val_loss: 0.2390 - learning_rate: 5.0000e-04\nEpoch 5/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9056 - loss: 0.2419 - val_accuracy: 0.9099 - val_loss: 0.2268 - learning_rate: 5.0000e-04\nEpoch 6/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9054 - loss: 0.2394 - val_accuracy: 0.9004 - val_loss: 0.2412 - learning_rate: 5.0000e-04\nEpoch 7/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9038 - loss: 0.2361 - val_accuracy: 0.9190 - val_loss: 0.2105 - learning_rate: 5.0000e-04\nEpoch 8/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9053 - loss: 0.2384 - val_accuracy: 0.9112 - val_loss: 0.2269 - learning_rate: 5.0000e-04\nEpoch 9/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9052 - loss: 0.2365 - val_accuracy: 0.9043 - val_loss: 0.2514 - learning_rate: 5.0000e-04\nEpoch 10/50\n\u001b[1m1380/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9075 - loss: 0.2329\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9075 - loss: 0.2330 - val_accuracy: 0.9089 - val_loss: 0.2230 - learning_rate: 5.0000e-04\nEpoch 11/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9064 - loss: 0.2326 - val_accuracy: 0.9044 - val_loss: 0.2436 - learning_rate: 2.5000e-04\nEpoch 12/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9087 - loss: 0.2311 - val_accuracy: 0.9080 - val_loss: 0.2376 - learning_rate: 2.5000e-04\nEpoch 13/50\n\u001b[1m1386/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9090 - loss: 0.2300\nEpoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9090 - loss: 0.2300 - val_accuracy: 0.9005 - val_loss: 0.2428 - learning_rate: 2.5000e-04\nEpoch 14/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9050 - loss: 0.2274 - val_accuracy: 0.9100 - val_loss: 0.2258 - learning_rate: 1.2500e-04\nEpoch 15/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9069 - loss: 0.2318 - val_accuracy: 0.9061 - val_loss: 0.2347 - learning_rate: 1.2500e-04\nEpoch 16/50\n\u001b[1m1392/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9087 - loss: 0.2281\nEpoch 16: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9087 - loss: 0.2281 - val_accuracy: 0.9063 - val_loss: 0.2320 - learning_rate: 1.2500e-04\nEpoch 17/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9082 - loss: 0.2263 - val_accuracy: 0.9118 - val_loss: 0.2192 - learning_rate: 6.2500e-05\n\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9187 - loss: 0.2092\nTest Accuracy: 0.9217320084571838\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import f1_score\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test).flatten()\n\nclasses = np.unique(y_train)\nclass_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\nclass_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n\nmodel = Sequential([\n    Input(shape=(X_train.shape[1],)),\n    Dense(512, activation='elu', kernel_regularizer=l2(0.001)),\n    BatchNormalization(),\n    \n    Dense(128, activation='elu', kernel_regularizer=l2(0.001)),\n    BatchNormalization(),\n    \n    Dense(64, activation='elu', kernel_regularizer=l2(0.001)),\n    BatchNormalization(),\n\n    \n    Dense(32, activation='elu'),\n    BatchNormalization(),\n    \n    Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\nearly_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=[early_stop], class_weight=class_weight_dict)\nloss, acc = model.evaluate(X_test, y_test)\nprint(\"Test Accuracy:\", acc)\ny_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\nf1 = f1_score(y_test, y_pred)\nprint(\"F1 Score:\", f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T19:43:41.317434Z","iopub.execute_input":"2025-02-04T19:43:41.317734Z","iopub.status.idle":"2025-02-04T19:45:11.411142Z","shell.execute_reply.started":"2025-02-04T19:43:41.317710Z","shell.execute_reply":"2025-02-04T19:45:11.410222Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8807 - loss: 0.4821 - val_accuracy: 0.9047 - val_loss: 0.2622\nEpoch 2/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9038 - loss: 0.2517 - val_accuracy: 0.9189 - val_loss: 0.2111\nEpoch 3/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9043 - loss: 0.2300 - val_accuracy: 0.9181 - val_loss: 0.2234\nEpoch 4/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9067 - loss: 0.2349 - val_accuracy: 0.8888 - val_loss: 0.3153\nEpoch 5/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9057 - loss: 0.2369 - val_accuracy: 0.9126 - val_loss: 0.2354\nEpoch 6/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9063 - loss: 0.2326 - val_accuracy: 0.8875 - val_loss: 0.3135\nEpoch 7/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9098 - loss: 0.2292 - val_accuracy: 0.9217 - val_loss: 0.2036\nEpoch 8/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9086 - loss: 0.2287 - val_accuracy: 0.9086 - val_loss: 0.2264\nEpoch 9/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9063 - loss: 0.2338 - val_accuracy: 0.9255 - val_loss: 0.1909\nEpoch 10/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9071 - loss: 0.2261 - val_accuracy: 0.9038 - val_loss: 0.2435\nEpoch 11/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9076 - loss: 0.2282 - val_accuracy: 0.8247 - val_loss: 0.5010\nEpoch 12/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9103 - loss: 0.2221 - val_accuracy: 0.9171 - val_loss: 0.2158\nEpoch 13/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9117 - loss: 0.2214 - val_accuracy: 0.9221 - val_loss: 0.1930\nEpoch 14/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9109 - loss: 0.2224 - val_accuracy: 0.9091 - val_loss: 0.2313\nEpoch 15/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9111 - loss: 0.2178 - val_accuracy: 0.9220 - val_loss: 0.1966\nEpoch 16/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9096 - loss: 0.2238 - val_accuracy: 0.8755 - val_loss: 0.3370\nEpoch 17/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9113 - loss: 0.2172 - val_accuracy: 0.8801 - val_loss: 0.3235\nEpoch 18/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9114 - loss: 0.2187 - val_accuracy: 0.9176 - val_loss: 0.2078\nEpoch 19/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9098 - loss: 0.2199 - val_accuracy: 0.9312 - val_loss: 0.1830\nEpoch 20/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9092 - loss: 0.2190 - val_accuracy: 0.9244 - val_loss: 0.1979\nEpoch 21/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9112 - loss: 0.2214 - val_accuracy: 0.9041 - val_loss: 0.2377\nEpoch 22/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9123 - loss: 0.2176 - val_accuracy: 0.9031 - val_loss: 0.2403\nEpoch 23/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9132 - loss: 0.2154 - val_accuracy: 0.9286 - val_loss: 0.1943\nEpoch 24/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9121 - loss: 0.2191 - val_accuracy: 0.9160 - val_loss: 0.2139\nEpoch 25/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9146 - loss: 0.2141 - val_accuracy: 0.9163 - val_loss: 0.2127\nEpoch 26/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9127 - loss: 0.2187 - val_accuracy: 0.9198 - val_loss: 0.2091\nEpoch 27/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9123 - loss: 0.2163 - val_accuracy: 0.9117 - val_loss: 0.2216\nEpoch 28/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9113 - loss: 0.2191 - val_accuracy: 0.9187 - val_loss: 0.2057\nEpoch 29/50\n\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9133 - loss: 0.2142 - val_accuracy: 0.9094 - val_loss: 0.2304\n\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9305 - loss: 0.1809\nTest Accuracy: 0.9342306852340698\n\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\nF1 Score: 0.8301921485703778\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Hyperparameter tuning for ANN","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import f1_score\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport keras_tuner as kt\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test).flatten()\nclasses = np.unique(y_train)\nclass_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\nclass_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n\ndef build_model(hp):\n    model = Sequential()\n    model.add(Input(shape=(X_train.shape[1],)))\n    model.add(Dense(units=hp.Choice('units1', values=[256, 512, 1024]), activation=hp.Choice('act1', values=['elu','relu']), kernel_regularizer=l2(hp.Choice('l2_1', values=[0.001, 0.0001]))))\n    model.add(BatchNormalization())\n    model.add(Dense(units=hp.Choice('units2', values=[64, 128, 256]), activation=hp.Choice('act2', values=['elu','relu']), kernel_regularizer=l2(hp.Choice('l2_2', values=[0.001, 0.0001]))))\n    model.add(BatchNormalization())\n    model.add(Dense(units=hp.Choice('units3', values=[32, 64, 128]), activation=hp.Choice('act3', values=['elu','relu']), kernel_regularizer=l2(hp.Choice('l2_3', values=[0.001, 0.0001]))))\n    model.add(BatchNormalization())\n    model.add(Dense(units=hp.Choice('units4', values=[16, 32, 64]), activation=hp.Choice('act4', values=['elu','relu'])))\n    model.add(BatchNormalization())\n    model.add(Dense(1, activation='sigmoid'))\n    lr = hp.Choice('lr', values=[0.001, 0.0005, 0.0001])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ntuner = kt.Hyperband(build_model, objective='val_accuracy', max_epochs=50, factor=3, directory='hyper_dir', project_name='hyper_tuning')\nstop_early = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\ntuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early], class_weight=class_weight_dict)\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nmodel = tuner.hypermodel.build(best_hps)\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_split=0.2, callbacks=[stop_early], class_weight=class_weight_dict)\nloss, acc = model.evaluate(X_test, y_test)\nprint(\"Test Accuracy:\", acc)\ny_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\nf1 = f1_score(y_test, y_pred)\nprint(\"F1 Score:\", f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T19:48:12.809463Z","iopub.execute_input":"2025-02-04T19:48:12.809764Z","iopub.status.idle":"2025-02-04T21:04:45.653831Z","shell.execute_reply.started":"2025-02-04T19:48:12.809741Z","shell.execute_reply":"2025-02-04T21:04:45.653091Z"}},"outputs":[{"name":"stdout","text":"Trial 90 Complete [00h 02m 31s]\nval_accuracy: 0.9233953356742859\n\nBest val_accuracy So Far: 0.9367488622665405\nTotal elapsed time: 01h 16m 05s\nEpoch 1/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8883 - loss: 0.5574 - val_accuracy: 0.9066 - val_loss: 0.2697\nEpoch 2/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9069 - loss: 0.2506 - val_accuracy: 0.9234 - val_loss: 0.1996\nEpoch 3/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9099 - loss: 0.2273 - val_accuracy: 0.9181 - val_loss: 0.2076\nEpoch 4/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9103 - loss: 0.2213 - val_accuracy: 0.9063 - val_loss: 0.2465\nEpoch 5/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9107 - loss: 0.2215 - val_accuracy: 0.9211 - val_loss: 0.2011\nEpoch 6/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9092 - loss: 0.2239 - val_accuracy: 0.9202 - val_loss: 0.2082\nEpoch 7/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9120 - loss: 0.2180 - val_accuracy: 0.9205 - val_loss: 0.2004\nEpoch 8/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9113 - loss: 0.2180 - val_accuracy: 0.9009 - val_loss: 0.2616\nEpoch 9/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9122 - loss: 0.2203 - val_accuracy: 0.8613 - val_loss: 0.4142\nEpoch 10/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9115 - loss: 0.2208 - val_accuracy: 0.8982 - val_loss: 0.2654\nEpoch 11/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9107 - loss: 0.2193 - val_accuracy: 0.9193 - val_loss: 0.2008\nEpoch 12/50\n\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9129 - loss: 0.2181 - val_accuracy: 0.9043 - val_loss: 0.2446\n\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9245 - loss: 0.1975\nTest Accuracy: 0.9282128214836121\n\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\nF1 Score: 0.8210227272727273\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport xgboost as xgb\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.fit_transform(X_test)\n\nmodel = xgb.XGBClassifier(\n    objective='binary:logistic',\n    booster='gbtree',\n    learning_rate=0.1,\n    n_estimators=100,\n    max_depth=6,\n    min_child_weight=1,\n    gamma=0,\n    subsample=1,\n    colsample_bytree=1,\n    reg_alpha=0,\n    reg_lambda=1,\n    random_state=42,\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(best_hps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T21:04:45.655002Z","iopub.execute_input":"2025-02-04T21:04:45.655296Z","iopub.status.idle":"2025-02-04T21:04:45.659321Z","shell.execute_reply.started":"2025-02-04T21:04:45.655274Z","shell.execute_reply":"2025-02-04T21:04:45.658522Z"}},"outputs":[{"name":"stdout","text":"<keras_tuner.src.engine.hyperparameters.hyperparameters.HyperParameters object at 0x785afa5772e0>\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport xgboost as xgb\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nsearch_spaces = {\n    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n    'n_estimators': Integer(50, 500),\n    'max_depth': Integer(3, 10),\n    'min_child_weight': Integer(1, 10),\n    'gamma': Real(0, 0.5),\n    'subsample': Real(0.5, 1.0),\n    'colsample_bytree': Real(0.5, 1.0),\n    'reg_alpha': Real(0, 1.0),\n    'reg_lambda': Real(0, 1.0)\n}\n\nmodel = xgb.XGBClassifier(\n    objective='binary:logistic',\n    booster='gbtree',\n    random_state=42,\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\n\nopt = BayesSearchCV(\n    estimator=model,\n    search_spaces=search_spaces,\n    scoring='accuracy',\n    cv=3,\n    n_iter=30,\n    random_state=42,\n    verbose=0\n)\n\nopt.fit(X_train, y_train)\n\nprint(\"Best parameters:\", opt.best_params_)\nprint(\"Best cross-validation accuracy: {:.4f}\".format(opt.best_score_))\n\ny_pred = opt.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(f'Test Accuracy: {accuracy:.4f}')\nprint(f'Test F1 Score: {f1:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T03:56:48.576539Z","iopub.execute_input":"2025-02-05T03:56:48.576974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}